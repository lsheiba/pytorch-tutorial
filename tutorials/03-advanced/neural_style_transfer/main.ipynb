{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image file and convert it into variable\n",
    "# unsqueeze for make the 4D tensor to perform conv arithmetic\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape is not None:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained VGGNet \n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract 5 conv activation maps from an input image.\n",
    "        \n",
    "        Args:\n",
    "            x: 4D tensor of shape (1, 3, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            features: a list containing 5 conv activation maps.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # For normalization, see https://github.com/pytorch/vision#models\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # make content.size() == style.size() \n",
    "    content = load_image(config.content, transform, max_size=config.max_size)\n",
    "    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialization and optimizer\n",
    "    target = Variable(content.clone(), requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    \n",
    "    vgg = VGGNet()\n",
    "    if use_cuda:\n",
    "        vgg.cuda()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(Variable(content))\n",
    "        style_features = vgg(Variable(style))\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss (target and content image)\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape conv features\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix  \n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss (target and style image)\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "\n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f' \n",
    "                   %(step+1, config.total_step, content_loss.data[0], style_loss.data[0]))\n",
    "    \n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().cpu().squeeze()\n",
    "            img = denorm(img.data).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-%d.png' %(step+1))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--content', type=str, default='./png/content.png')\n",
    "    parser.add_argument('--style', type=str, default='./png/style.png')\n",
    "    parser.add_argument('--max_size', type=int, default=400)\n",
    "    parser.add_argument('--total_step', type=int, default=5000)\n",
    "    parser.add_argument('--log_step', type=int, default=10)\n",
    "    parser.add_argument('--sample_step', type=int, default=1000)\n",
    "    parser.add_argument('--style_weight', type=float, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.003)\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
